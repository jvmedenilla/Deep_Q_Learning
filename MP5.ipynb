{"cells":[{"cell_type":"markdown","metadata":{"id":"b9Q3yESzJ3XJ"},"source":["# Deep Q-Learning "]},{"cell_type":"markdown","metadata":{"id":"nOmbeN4PJ3XQ"},"source":["Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvCP5ccDJ4ZV","executionInfo":{"status":"ok","timestamp":1651608500658,"user_tz":300,"elapsed":1222,"user":{"displayName":"Jon Vincent Medenilla","userId":"18093056255900426440"}},"outputId":"c4d74fba-e075-4b2e-fb7c-bd608828ec27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/444_MP5/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xH6YvwCbJ4Um","executionInfo":{"status":"ok","timestamp":1651608500659,"user_tz":300,"elapsed":25,"user":{"displayName":"Jon Vincent Medenilla","userId":"18093056255900426440"}},"outputId":"c5104cff-eac7-446f-d26d-21253fe389e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/444_MP5\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/444_MP5/\")\n","import sys\n","sys.path.append(\".\")\n","%autosave 60"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"v9wFasXVp5Fw","executionInfo":{"status":"ok","timestamp":1651608500661,"user_tz":300,"elapsed":22,"user":{"displayName":"Jon Vincent Medenilla","userId":"18093056255900426440"}},"outputId":"d5aa4b31-cbc9-4797-c09b-e0215f048efd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["IPython.notebook.set_autosave_interval(60000)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Autosaving every 60 seconds\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-wB2gWPJ4O-","executionInfo":{"status":"ok","timestamp":1651608500662,"user_tz":300,"elapsed":19,"user":{"displayName":"Jon Vincent Medenilla","userId":"18093056255900426440"}},"outputId":"bd151788-df1e-47d7-d938-bc81ed6e52aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["agent_double.py  __MACOSX   MP5.ipynb\t Roms.zip    utils.py\n","agent.py\t memory.py  __pycache__  save_graph\n","config.py\t model.py   Roms.rar\t save_model\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAmgOVbaJ3XS"},"outputs":[],"source":["#!pip3 install gym pyvirtualdisplay\n","#!sudo apt-get install -y xvfb python-opengl ffmpeg"]},{"cell_type":"code","source":["#!pip3  install tf-estimator-nightly==2.8.0.dev2021122109"],"metadata":{"id":"Qn7j9MSMKUm7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!python -m pip install --upgrade pip"],"metadata":{"id":"70oS32eqXSyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvBfzwcoJ3XU"},"outputs":[],"source":["#!pip3 install --upgrade setuptools --user\n","#!pip3 install ez_setup \n","#!pip3 install gym[atari] "]},{"cell_type":"markdown","metadata":{"id":"Wuf3qe52J3XV"},"source":["For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G05T0zywJ3XW"},"outputs":[],"source":["%matplotlib inline\n","\n","import sys\n","import gym\n","import torch\n","import pylab\n","import random\n","import numpy as np\n","from collections import deque\n","from datetime import datetime\n","from copy import deepcopy\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from utils import find_max_lives, check_live, get_frame, get_init_state\n","from model import DQN, DQN_LSTM\n","from config import *\n","\n","import matplotlib.pyplot as plt\n","# %load_ext autoreload\n","# %autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"9HlAfLAlJ3XX"},"source":["## Understanding the environment"]},{"cell_type":"markdown","metadata":{"id":"WJ5TeQk9J3XY"},"source":["In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n","\n","In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."]},{"cell_type":"code","source":["#!pip3 install atari-py"],"metadata":{"id":"Gn1WteBpKrIm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!python3 -m atari_py.import_roms '/content/drive/MyDrive/444_MP5/'"],"metadata":{"id":"xWOSGNSHKuvR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElNgebDaJ3Xa"},"outputs":[],"source":["env = gym.make('BreakoutDeterministic-v4')\n","state = env.reset()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPgFQAXdJ3Xb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651608503679,"user_tz":300,"elapsed":15,"user":{"displayName":"Jon Vincent Medenilla","userId":"18093056255900426440"}},"outputId":"45e68621-99c0-4d4f-c028-0de983beed15"},"outputs":[{"output_type":"stream","name":"stdout","text":["9\n"]}],"source":["number_lives = find_max_lives(env)\n","state_size = env.observation_space.shape\n","action_size = action_size = env.action_space.n\n","print(action_size)\n"]},{"cell_type":"markdown","metadata":{"id":"a0p_Ks8WJ3Xc"},"source":["## Creating a DQN Agent"]},{"cell_type":"markdown","metadata":{"id":"CPz1RguPJ3Xc"},"source":["Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n","\n","__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n","\n","__Frame__ : Number of frames processed in total.\n","\n","__Memory Size__ : The current size of the replay memory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYmjmn6-J3Xd"},"outputs":[],"source":["double_dqn = True # set to True if using double DQN agent\n","\n","if double_dqn:\n","    from agent_double import Agent\n","else:\n","    from agent import Agent\n","\n","agent = Agent(action_size)\n","evaluation_reward = deque(maxlen=evaluation_reward_length)\n","frame = 0\n","memory_size = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":506},"id":"2lG6igmKJ3Xe","executionInfo":{"status":"ok","timestamp":1651608773476,"user_tz":300,"elapsed":266213,"user":{"displayName":"Jon Vincent Medenilla","userId":"18093056255900426440"}},"outputId":"e54e65af-6202-4f72-a80f-01c26b17b083"},"outputs":[{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/444_MP5/memory.py:38: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n","  sample = np.array(sample)\n","/content/drive/MyDrive/444_MP5/memory.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  sample = np.array(sample)\n","/content/drive/MyDrive/444_MP5/agent_double.py:77: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n","  mini_batch = np.array(mini_batch).transpose()\n","/content/drive/MyDrive/444_MP5/agent_double.py:77: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  mini_batch = np.array(mini_batch).transpose()\n"]},{"output_type":"stream","name":"stdout","text":["episode: 0   score: 308.0   memory length: 1904   epsilon: 0.9964261000000776    steps: 1904    lr: 0.001     evaluation reward: 308.0\n","episode: 1   score: 396.0   memory length: 4156   epsilon: 0.9919671400001744    steps: 2252    lr: 0.001     evaluation reward: 352.0\n","episode: 2   score: 484.0   memory length: 7191   epsilon: 0.9859578400003048    steps: 3035    lr: 0.001     evaluation reward: 396.0\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["rewards, episodes = [], []\n","best_eval_reward = 0\n","for e in range(EPISODES):\n","    done = False\n","    score = 0\n","\n","    history = np.zeros([5, 84, 84], dtype=np.uint8)\n","    step = 0\n","    d = False\n","    state = env.reset()\n","    next_state = state\n","    life = number_lives\n","\n","    get_init_state(history, state, HISTORY_SIZE)\n","\n","    while not done:\n","        step += 1\n","        frame += 1\n","\n","        # Perform a fire action if ball is no longer on screen to continue onto next life\n","        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n","            action = 0\n","        else:\n","            #print(np.float32(history[:4, :, :]) / 255.)\n","            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n","        state = next_state\n","        next_state, reward, done, info = env.step(action)\n","        \n","        frame_next_state = get_frame(next_state)\n","        history[4, :, :] = frame_next_state\n","        terminal_state = check_live(life, info['ale.lives'])\n","\n","        life = info['ale.lives']\n","        r = np.clip(reward, -1, 1) \n","        r = reward\n","\n","        # Store the transition in memory \n","        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n","        # Start training after random sample generation\n","        if(frame >= train_frame):\n","            agent.train_policy_net(frame)\n","            # Update the target network only for Double DQN only\n","            if double_dqn and (frame % update_target_network_frequency)== 0:\n","                agent.update_target_net()\n","        score += reward\n","        history[:4, :, :] = history[1:, :, :]\n","            \n","        if done:\n","            evaluation_reward.append(score)\n","            rewards.append(np.mean(evaluation_reward))\n","            episodes.append(e)\n","            pylab.plot(episodes, rewards, 'b')\n","            pylab.xlabel('Episodes')\n","            pylab.ylabel('Rewards') \n","            pylab.title('Episodes vs Reward')\n","            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n","            \n","            # every episode, plot the play time\n","            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n","                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n","                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n","\n","            # if the mean of scores of last 100 episode is bigger than 5 save model\n","            ### Change this save condition to whatever you prefer ###\n","            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n","                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n","                best_eval_reward = np.mean(evaluation_reward)\n"]},{"cell_type":"markdown","metadata":{"id":"vhXv5wdWJ3Xd"},"source":["### Main Training Loop"]},{"cell_type":"markdown","metadata":{"id":"opcCdaEZJ3Xe"},"source":["In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-Ybk3wcJ3Xg"},"outputs":[],"source":["from agent import LSTM_Agent\n","agent = LSTM_Agent(action_size)\n","evaluation_reward = deque(maxlen=evaluation_reward_length)\n","frame = 0\n","memory_size = 0\n","\n","HISTORY_SIZE = 1\n","rewards, episodes = [], []\n","best_eval_reward = 0\n","for e in range(EPISODES):\n","    done = False\n","    score = 0\n","\n","    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n","    step = 0\n","    d = False\n","    state = env.reset()\n","    next_state = state\n","    life = number_lives\n","    hidden = None\n","\n","    get_init_state(history, state, HISTORY_SIZE)\n","\n","    while not done:\n","        step += 1\n","        frame += 1\n","\n","        # Perform a fire action if ball is no longer on screen to continue onto next life\n","        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n","            action = 0\n","        else:\n","            action, hidden = agent.get_action(np.float32(history[:1, :, :]) / 255., hidden)\n","        state = next_state\n","        next_state, reward, done, info = env.step(action + 1)\n","        \n","        frame_next_state = get_frame(next_state)\n","        history[1, :, :] = frame_next_state\n","        terminal_state = check_live(life, info['ale.lives'])\n","\n","        life = info['ale.lives']\n","        r = np.clip(reward, -1, 1) \n","        r = reward\n","\n","        # Store the transition in memory \n","        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n","        # Start training after random sample generation\n","        if(frame >= train_frame):\n","            agent.train_policy_net(frame)\n","            # Update the target network only for Double DQN only\n","            if double_dqn and (frame % update_target_network_frequency)== 0:\n","                agent.update_target_net()\n","        score += reward\n","        history[:1, :, :] = history[1:, :, :]\n","            \n","        if done:\n","            evaluation_reward.append(score)\n","            rewards.append(np.mean(evaluation_reward))\n","            episodes.append(e)\n","            pylab.plot(episodes, rewards, 'b')\n","            pylab.xlabel('Episodes')\n","            pylab.ylabel('Rewards') \n","            pylab.title('Episodes vs Reward')\n","            pylab.savefig(\"./save_graph/breakout_dqn_lstm.png\") # save graph for training visualization\n","            \n","            # every episode, plot the play time\n","            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n","                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n","                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n","\n","            # if the mean of scores of last 100 episode is bigger than 5 save model\n","            ### Change this save condition to whatever you prefer ###\n","            if np.mean(evaluation_reward) > 1: # and np.mean(evaluation_reward) > best_eval_reward:\n","                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n","                best_eval_reward = np.mean(evaluation_reward)\n"]},{"cell_type":"code","source":["load_network_path = \"./save_model/breakout_dqn.pth\"\n","if load_network_path is not None:\n","    print('Loading saved network from {}'.format(load_network_path))\n","    agent.policy_net = torch.load(load_network_path)"],"metadata":{"id":"DXitB1M9zalP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for e in range(EPISODES):\n","    done = False\n","    score = 0\n","\n","    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n","    step = 0\n","    d = False\n","    state = env.reset()\n","    next_state = state\n","    life = number_lives\n","    hidden = None\n","\n","    get_init_state(history, state, HISTORY_SIZE)\n","\n","    while not done:\n","        step += 1\n","        frame += 1\n","\n","        # Perform a fire action if ball is no longer on screen to continue onto next life\n","        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n","            action = 0\n","        else:\n","            action, hidden = agent.get_action(np.float32(history[:1, :, :]) / 255., hidden)\n","        state = next_state\n","        next_state, reward, done, info = env.step(action + 1)\n","        \n","        frame_next_state = get_frame(next_state)\n","        history[1, :, :] = frame_next_state\n","        terminal_state = check_live(life, info['ale.lives'])\n","\n","        life = info['ale.lives']\n","        r = np.clip(reward, -1, 1) \n","        r = reward\n","\n","        # Store the transition in memory \n","        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n","        # Start training after random sample generation\n","        if(frame >= train_frame):\n","            agent.train_policy_net(frame)\n","            # Update the target network only for Double DQN only\n","            if double_dqn and (frame % update_target_network_frequency)== 0:\n","                agent.update_target_net()\n","        score += reward\n","        history[:1, :, :] = history[1:, :, :]\n","            \n","        if done:\n","            evaluation_reward.append(score)\n","            rewards.append(np.mean(evaluation_reward))\n","            episodes.append(e)\n","            pylab.plot(episodes, rewards, 'b')\n","            pylab.xlabel('Episodes')\n","            pylab.ylabel('Rewards') \n","            pylab.title('Episodes vs Reward')\n","            pylab.savefig(\"./save_graph/breakout_dqn_lstm.png\") # save graph for training visualization\n","            \n","            # every episode, plot the play time\n","            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n","                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n","                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n","\n","            # if the mean of scores of last 100 episode is bigger than 5 save model\n","            ### Change this save condition to whatever you prefer ###\n","            if np.mean(evaluation_reward) > 1: # and np.mean(evaluation_reward) > best_eval_reward:\n","                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n","                best_eval_reward = np.mean(evaluation_reward)"],"metadata":{"id":"moyu0Gmn17q3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = np.zeros((32,20,84,84))\n","z = torch.tensor(y)\n","z = torch.reshape(z, (32*20, 1, 84,84))\n","z = z.reshape(32,20,84,84)\n","print(z.size())\n","print(z[:,-1:,:,:].shape)\n","\n","#print(z[:,[0],:,:].size())"],"metadata":{"id":"UbgmzhGhajeB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mK_Yl4JUJ3Xf"},"source":["# Creating a DQN LSTM Agent"]},{"cell_type":"markdown","metadata":{"id":"OrlLzFTtJ3Xf"},"source":["Now we will create a DQN agent that uses LSTM rather than past frames as history. We augment the experience replay to contain previous few (state, action, reward, next states) tuples rather than just one (state, action, reward, next states) tuple. This previous few states will enable the LSTM encode the history. Training loop remains nearly the same"]},{"cell_type":"markdown","metadata":{"id":"YW0y7tRmJ3Xh"},"source":["# Visualize Agent Performance"]},{"cell_type":"markdown","metadata":{"id":"_UAeDcF5J3Xh"},"source":["BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n","\n","Please save your model before running this portion of the code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VByQW3okJ3Xh"},"outputs":[],"source":["torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1Fn_JpBJ3Xh"},"outputs":[],"source":["from gym.wrappers import Monitor\n","import glob\n","import io\n","import base64\n","\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","\n","from pyvirtualdisplay import Display\n","\n","# Displaying the game live\n","def show_state(env, step=0, info=\"\"):\n","    plt.figure(3)\n","    plt.clf()\n","    plt.imshow(env.render(mode='rgb_array'))\n","    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n","    plt.axis('off')\n","\n","    ipythondisplay.clear_output(wait=True)\n","    ipythondisplay.display(plt.gcf())\n","    \n","# Recording the game and replaying the game afterwards\n","def show_video():\n","    mp4list = glob.glob('video/*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else: \n","        print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","    env = Monitor(env, './video', force=True)\n","    return env"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARoyW1uRJ3Xi"},"outputs":[],"source":["display = Display(visible=0, size=(300, 200))\n","display.start()\n","\n","# Load agent\n","# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n","agent.epsilon = 0.0 # Set agent to only exploit the best action\n","\n","env = gym.make('BreakoutDeterministic-v4')\n","env = wrap_env(env)\n","\n","done = False\n","score = 0\n","step = 0\n","state = env.reset()\n","next_state = state\n","life = number_lives\n","history = np.zeros([5, 84, 84], dtype=np.uint8)\n","get_init_state(history, state)\n","\n","while not done:\n","    \n","    # Render breakout\n","    env.render()\n","#     show_state(env,step) # uncommenting this provides another way to visualize the game\n","\n","    step += 1\n","    frame += 1\n","\n","    # Perform a fire action if ball is no longer on screen\n","    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n","        action = 0\n","    else:\n","        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n","    state = next_state\n","    \n","    next_state, reward, done, info = env.step(action + 1)\n","        \n","    frame_next_state = get_frame(next_state)\n","    history[4, :, :] = frame_next_state\n","    terminal_state = check_live(life, info['ale.lives'])\n","        \n","    life = info['ale.lives']\n","    r = np.clip(reward, -1, 1) \n","    r = reward\n","\n","    # Store the transition in memory \n","    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n","    # Start training after random sample generation\n","    score += reward\n","    \n","    history[:4, :, :] = history[1:, :, :]\n","env.close()\n","show_video()\n","display.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiqOxsdRJ3Xi"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}